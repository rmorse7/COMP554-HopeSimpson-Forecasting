{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Province_State    Country_Region        Lat        Long    Date  \\\n",
      "96531              Yemen             Yemen  15.552727   48.516388  1/1/21   \n",
      "96432           Maldives          Maldives   3.202800   73.220700  1/1/21   \n",
      "96433               Mali              Mali  17.570692   -3.996166  1/1/21   \n",
      "96434              Malta             Malta  35.937500   14.375400  1/1/21   \n",
      "96435   Marshall Islands  Marshall Islands   7.131500  171.184500  1/1/21   \n",
      "...                  ...               ...        ...         ...     ...   \n",
      "166383              Cuba              Cuba  21.521757  -77.781167  9/9/21   \n",
      "166382           Croatia           Croatia  45.100000   15.200000  9/9/21   \n",
      "166381     Cote d'Ivoire     Cote d'Ivoire   7.540000   -5.547100  9/9/21   \n",
      "166387         Greenland           Denmark  71.706900  -42.604300  9/9/21   \n",
      "166285           Albania           Albania  41.153300   20.168300  9/9/21   \n",
      "\n",
      "        Confirmed Cases  \n",
      "96531               2.0  \n",
      "96432              47.0  \n",
      "96433             113.0  \n",
      "96434             135.0  \n",
      "96435               0.0  \n",
      "...                 ...  \n",
      "166383           7747.0  \n",
      "166382            941.0  \n",
      "166381            131.0  \n",
      "166387              0.0  \n",
      "166285            998.0  \n",
      "\n",
      "[177654 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "raw_global_df = pd.read_csv('./covid_global.csv').dropna()\n",
    "global_df = raw_global_df.drop(raw_global_df[raw_global_df['Confirmed Cases'] < 0].index)\n",
    "global_df = global_df.sort_values(by='Date', ascending=True)\n",
    "print(global_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "data_dict = defaultdict(list)\n",
    "\n",
    "for _, line in global_df.iterrows():\n",
    "    prov_sta, cty_reg = line['Province_State'], line['Country_Region']\n",
    "    data = [line['Lat'], line['Long'], line['Confirmed Cases']]\n",
    "    data_dict[prov_sta, cty_reg].append(data)\n",
    "for k in data_dict.keys():\n",
    "    data_dict[k] = np.array(data_dict[k])\n",
    "print(len(data_dict['Yemen', 'Yemen']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160613, 30, 3) (8454, 30, 3)\n",
      "(160613, 3) (8454, 3)\n"
     ]
    }
   ],
   "source": [
    "# Construct the training and testing data\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "WINDOW_SIZE = 30\n",
    "for key, item in data_dict.items():\n",
    "    N = item.shape[0]\n",
    "    for i in range(0, N - WINDOW_SIZE - 1, 1):\n",
    "        start, end = i, i + WINDOW_SIZE\n",
    "        inputs = item[start: end, :]  # index -1 is the case number\n",
    "        label = item[end + 1, :]\n",
    "        X.append(inputs)\n",
    "        y.append(label)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "RAND_SEED = 2021\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=RAND_SEED)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 30, 3)             12        \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                4608      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 5,775\n",
      "Trainable params: 5,769\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Construct and compile the LSTM neural network regression model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(units=32))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(3))\n",
    "\n",
    "model.compile(loss='mae', optimizer='nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1130/1130 [==============================] - 9s 8ms/step - loss: 463.9566 - val_loss: 448.9614\n",
      "Epoch 2/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 454.3086 - val_loss: 439.6700\n",
      "Epoch 3/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 440.2695 - val_loss: 420.4601\n",
      "Epoch 4/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 421.9083 - val_loss: 401.7033\n",
      "Epoch 5/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 396.7168 - val_loss: 364.2455\n",
      "Epoch 6/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 366.2750 - val_loss: 338.5158\n",
      "Epoch 7/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 349.5677 - val_loss: 328.1284\n",
      "Epoch 8/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 337.9878 - val_loss: 313.6724\n",
      "Epoch 9/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 327.0066 - val_loss: 302.4486\n",
      "Epoch 10/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 319.9662 - val_loss: 294.5625\n",
      "Epoch 11/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 313.3898 - val_loss: 287.5759\n",
      "Epoch 12/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 308.0549 - val_loss: 280.4359\n",
      "Epoch 13/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 301.1323 - val_loss: 277.5566\n",
      "Epoch 14/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 300.0712 - val_loss: 271.9005\n",
      "Epoch 15/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 295.0755 - val_loss: 281.3898\n",
      "Epoch 16/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 291.4395 - val_loss: 271.9286\n",
      "Epoch 17/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 286.8399 - val_loss: 263.3948\n",
      "Epoch 18/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 283.0398 - val_loss: 264.5700\n",
      "Epoch 19/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 283.2603 - val_loss: 253.9762\n",
      "Epoch 20/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 278.9713 - val_loss: 256.2984\n",
      "Epoch 21/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 278.1226 - val_loss: 256.7080\n",
      "Epoch 22/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 274.6826 - val_loss: 256.1664\n",
      "Epoch 23/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 273.0077 - val_loss: 261.1820\n",
      "Epoch 24/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 273.8680 - val_loss: 255.7816\n",
      "Epoch 25/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 269.7743 - val_loss: 248.1465\n",
      "Epoch 26/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 266.3336 - val_loss: 246.2191\n",
      "Epoch 27/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 265.6029 - val_loss: 246.4160\n",
      "Epoch 28/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 264.6385 - val_loss: 245.8941\n",
      "Epoch 29/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 263.5713 - val_loss: 234.4244\n",
      "Epoch 30/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 267.8593 - val_loss: 249.5859\n",
      "Epoch 31/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 266.6711 - val_loss: 240.2620\n",
      "Epoch 32/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 262.4591 - val_loss: 238.8364\n",
      "Epoch 33/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 261.5895 - val_loss: 237.8666\n",
      "Epoch 34/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 259.8873 - val_loss: 237.9930\n",
      "Epoch 35/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 256.4702 - val_loss: 229.3430\n",
      "Epoch 36/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 256.5887 - val_loss: 233.8158\n",
      "Epoch 37/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 254.9066 - val_loss: 230.1868\n",
      "Epoch 38/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 252.1470 - val_loss: 228.7004\n",
      "Epoch 39/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 251.8160 - val_loss: 230.5094\n",
      "Epoch 40/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 250.3973 - val_loss: 227.6572\n",
      "Epoch 41/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 247.8372 - val_loss: 223.2503\n",
      "Epoch 42/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 246.7498 - val_loss: 224.4562\n",
      "Epoch 43/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 247.0245 - val_loss: 227.4445\n",
      "Epoch 44/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 247.5936 - val_loss: 216.9416\n",
      "Epoch 45/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 248.5968 - val_loss: 228.4155\n",
      "Epoch 46/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 244.1197 - val_loss: 221.5484\n",
      "Epoch 47/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 243.8919 - val_loss: 223.5244\n",
      "Epoch 48/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 244.4035 - val_loss: 215.2167\n",
      "Epoch 49/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 241.0238 - val_loss: 220.0514\n",
      "Epoch 50/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 240.4195 - val_loss: 220.1579\n",
      "Epoch 51/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 238.4122 - val_loss: 216.8735\n",
      "Epoch 52/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 237.8329 - val_loss: 217.5754\n",
      "Epoch 53/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 236.2069 - val_loss: 219.5650\n",
      "Epoch 54/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 237.8911 - val_loss: 209.2335\n",
      "Epoch 55/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 236.6448 - val_loss: 208.7956\n",
      "Epoch 56/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 235.5592 - val_loss: 209.5551\n",
      "Epoch 57/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 236.9590 - val_loss: 211.1662\n",
      "Epoch 58/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 232.5637 - val_loss: 211.1942\n",
      "Epoch 59/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 234.9351 - val_loss: 213.1141\n",
      "Epoch 60/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 233.5598 - val_loss: 211.4263\n",
      "Epoch 61/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 232.7730 - val_loss: 206.9681\n",
      "Epoch 62/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 231.8689 - val_loss: 205.0027\n",
      "Epoch 63/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 231.2753 - val_loss: 209.9849\n",
      "Epoch 64/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 233.9909 - val_loss: 215.7695\n",
      "Epoch 65/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 230.6911 - val_loss: 206.3589\n",
      "Epoch 66/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 228.8929 - val_loss: 208.4769\n",
      "Epoch 67/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 227.6696 - val_loss: 204.1023\n",
      "Epoch 68/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 232.1009 - val_loss: 209.0746\n",
      "Epoch 69/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 226.6279 - val_loss: 204.7591\n",
      "Epoch 70/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 224.9797 - val_loss: 204.8072\n",
      "Epoch 71/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 227.0900 - val_loss: 207.7471\n",
      "Epoch 72/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 228.3436 - val_loss: 207.5072\n",
      "Epoch 73/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 228.4014 - val_loss: 204.7685\n",
      "Epoch 74/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 228.2556 - val_loss: 205.4466\n",
      "Epoch 75/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 228.2549 - val_loss: 204.8465\n",
      "Epoch 76/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 224.9728 - val_loss: 206.4761\n",
      "Epoch 77/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 224.0547 - val_loss: 201.2675\n",
      "Epoch 78/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 226.0024 - val_loss: 202.2945\n",
      "Epoch 79/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 226.6825 - val_loss: 203.2378\n",
      "Epoch 80/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 221.8339 - val_loss: 201.1503\n",
      "Epoch 81/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 223.7436 - val_loss: 198.4539\n",
      "Epoch 82/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 223.0380 - val_loss: 199.9166\n",
      "Epoch 83/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 219.4030 - val_loss: 201.5771\n",
      "Epoch 84/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 222.9648 - val_loss: 201.5881\n",
      "Epoch 85/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 226.1758 - val_loss: 200.7343\n",
      "Epoch 86/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 219.3362 - val_loss: 205.4199\n",
      "Epoch 87/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 220.2463 - val_loss: 193.3780\n",
      "Epoch 88/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 217.8531 - val_loss: 198.3009\n",
      "Epoch 89/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 221.6797 - val_loss: 198.2758\n",
      "Epoch 90/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 220.3631 - val_loss: 201.0628\n",
      "Epoch 91/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 220.3019 - val_loss: 198.2759\n",
      "Epoch 92/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 220.0457 - val_loss: 200.0925\n",
      "Epoch 93/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 220.3941 - val_loss: 194.8853\n",
      "Epoch 94/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 222.5420 - val_loss: 199.7953\n",
      "Epoch 95/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 218.6506 - val_loss: 194.7646\n",
      "Epoch 96/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 217.9057 - val_loss: 195.0690\n",
      "Epoch 97/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 217.3645 - val_loss: 196.9980\n",
      "Epoch 98/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 218.3649 - val_loss: 200.2455\n",
      "Epoch 99/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 219.2248 - val_loss: 203.9199\n",
      "Epoch 100/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 218.1581 - val_loss: 199.7623\n",
      "Epoch 101/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 218.0290 - val_loss: 194.2948\n",
      "Epoch 102/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 218.3196 - val_loss: 199.1389\n",
      "Epoch 103/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 222.9069 - val_loss: 196.9889\n",
      "Epoch 104/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 218.3391 - val_loss: 196.0522\n",
      "Epoch 105/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 217.2414 - val_loss: 193.7941\n",
      "Epoch 106/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 216.5206 - val_loss: 201.3020\n",
      "Epoch 107/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 216.6168 - val_loss: 195.5113\n",
      "Epoch 108/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 215.3280 - val_loss: 193.1588\n",
      "Epoch 109/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 214.4786 - val_loss: 194.9371\n",
      "Epoch 110/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 216.5950 - val_loss: 199.1329\n",
      "Epoch 111/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 214.2419 - val_loss: 195.2296\n",
      "Epoch 112/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 213.5907 - val_loss: 189.1183\n",
      "Epoch 113/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 215.1459 - val_loss: 190.5852\n",
      "Epoch 114/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 214.8397 - val_loss: 196.5739\n",
      "Epoch 115/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 214.1971 - val_loss: 194.5007\n",
      "Epoch 116/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 214.9111 - val_loss: 202.2338\n",
      "Epoch 117/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 214.7155 - val_loss: 196.9209\n",
      "Epoch 118/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 217.5472 - val_loss: 193.7684\n",
      "Epoch 119/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 212.5449 - val_loss: 195.5523\n",
      "Epoch 120/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 213.4301 - val_loss: 192.8920\n",
      "Epoch 121/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 212.7550 - val_loss: 196.7905\n",
      "Epoch 122/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 213.8208 - val_loss: 191.0004\n",
      "Epoch 123/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 216.5914 - val_loss: 193.0095\n",
      "Epoch 124/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 214.1852 - val_loss: 194.1427\n",
      "Epoch 125/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 210.5292 - val_loss: 190.6829\n",
      "Epoch 126/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 211.1550 - val_loss: 193.5010\n",
      "Epoch 127/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 208.8949 - val_loss: 193.3943\n",
      "Epoch 128/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 211.5372 - val_loss: 189.4102\n",
      "Epoch 129/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 210.2124 - val_loss: 190.7126\n",
      "Epoch 130/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 209.5208 - val_loss: 194.3073\n",
      "Epoch 131/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 215.2910 - val_loss: 190.1366\n",
      "Epoch 132/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 210.7893 - val_loss: 190.9284\n",
      "Epoch 133/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 211.5744 - val_loss: 192.4144\n",
      "Epoch 134/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 209.8776 - val_loss: 193.2605\n",
      "Epoch 135/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 209.1293 - val_loss: 194.8437\n",
      "Epoch 136/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 209.5335 - val_loss: 193.0998\n",
      "Epoch 137/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 207.8073 - val_loss: 188.6857\n",
      "Epoch 138/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 209.5527 - val_loss: 190.6504\n",
      "Epoch 139/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 206.3113 - val_loss: 189.1072\n",
      "Epoch 140/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 209.9297 - val_loss: 191.6816\n",
      "Epoch 141/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 209.7201 - val_loss: 190.9623\n",
      "Epoch 142/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 207.5663 - val_loss: 188.2714\n",
      "Epoch 143/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 207.2530 - val_loss: 195.8315\n",
      "Epoch 144/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 208.5903 - val_loss: 190.0233\n",
      "Epoch 145/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 209.2535 - val_loss: 188.6159\n",
      "Epoch 146/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 207.1769 - val_loss: 187.2427\n",
      "Epoch 147/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 205.1395 - val_loss: 189.6661\n",
      "Epoch 148/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 207.3865 - val_loss: 192.4862\n",
      "Epoch 149/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 207.9483 - val_loss: 190.0705\n",
      "Epoch 150/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 213.1372 - val_loss: 190.8455\n",
      "Epoch 151/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 207.6494 - val_loss: 186.9529\n",
      "Epoch 152/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 206.4054 - val_loss: 189.1633\n",
      "Epoch 153/400\n",
      "1130/1130 [==============================] - 10s 9ms/step - loss: 208.9818 - val_loss: 200.9450\n",
      "Epoch 154/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 209.0558 - val_loss: 190.4308\n",
      "Epoch 155/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 207.8345 - val_loss: 193.6238\n",
      "Epoch 156/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 205.8945 - val_loss: 192.5479\n",
      "Epoch 157/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 205.5958 - val_loss: 199.5774\n",
      "Epoch 158/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 210.4172 - val_loss: 190.4615\n",
      "Epoch 159/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 205.8691 - val_loss: 188.0118\n",
      "Epoch 160/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 203.5573 - val_loss: 193.3497\n",
      "Epoch 161/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 206.1257 - val_loss: 191.6742\n",
      "Epoch 162/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 206.8653 - val_loss: 190.0491\n",
      "Epoch 163/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 201.5363 - val_loss: 190.4285\n",
      "Epoch 164/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 205.2189 - val_loss: 189.5805\n",
      "Epoch 165/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 206.2734 - val_loss: 190.7049\n",
      "Epoch 166/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 204.7643 - val_loss: 191.5381\n",
      "Epoch 167/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 202.9218 - val_loss: 189.4969\n",
      "Epoch 168/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 203.2868 - val_loss: 190.5113\n",
      "Epoch 169/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 203.0867 - val_loss: 187.2103\n",
      "Epoch 170/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 207.4600 - val_loss: 203.1613\n",
      "Epoch 171/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 204.3088 - val_loss: 187.3613\n",
      "Epoch 172/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 202.2552 - val_loss: 187.5807\n",
      "Epoch 173/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 203.0899 - val_loss: 187.5622\n",
      "Epoch 174/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 206.7044 - val_loss: 189.2579\n",
      "Epoch 175/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 203.0197 - val_loss: 184.7215\n",
      "Epoch 176/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 203.7042 - val_loss: 185.9392\n",
      "Epoch 177/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 203.9652 - val_loss: 184.3476\n",
      "Epoch 178/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 202.4482 - val_loss: 186.7545\n",
      "Epoch 179/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 205.1219 - val_loss: 186.8564\n",
      "Epoch 180/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 201.5323 - val_loss: 191.4019\n",
      "Epoch 181/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 203.0583 - val_loss: 190.2778\n",
      "Epoch 182/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 205.6915 - val_loss: 194.0243\n",
      "Epoch 183/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 202.8176 - val_loss: 190.4784\n",
      "Epoch 184/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 201.0564 - val_loss: 187.4167\n",
      "Epoch 185/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 202.7938 - val_loss: 186.2340\n",
      "Epoch 186/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 202.4290 - val_loss: 182.9932\n",
      "Epoch 187/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 205.6754 - val_loss: 187.8763\n",
      "Epoch 188/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 201.9318 - val_loss: 188.9896\n",
      "Epoch 189/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 202.7687 - val_loss: 186.6994\n",
      "Epoch 190/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 201.3893 - val_loss: 187.1689\n",
      "Epoch 191/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 199.5630 - val_loss: 191.6550\n",
      "Epoch 192/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 201.8430 - val_loss: 184.3535\n",
      "Epoch 193/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 201.4503 - val_loss: 186.9422\n",
      "Epoch 194/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 199.2709 - val_loss: 187.8142\n",
      "Epoch 195/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 201.4338 - val_loss: 185.4326\n",
      "Epoch 196/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 198.9765 - val_loss: 184.5404\n",
      "Epoch 197/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 198.2912 - val_loss: 188.5915\n",
      "Epoch 198/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 199.6968 - val_loss: 188.9706\n",
      "Epoch 199/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 199.8219 - val_loss: 185.5637\n",
      "Epoch 200/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 200.1263 - val_loss: 187.1179\n",
      "Epoch 201/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 200.3753 - val_loss: 185.5020\n",
      "Epoch 202/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 198.2897 - val_loss: 184.8757\n",
      "Epoch 203/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 199.2898 - val_loss: 185.0338\n",
      "Epoch 204/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 198.0294 - val_loss: 185.6842\n",
      "Epoch 205/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 195.9694 - val_loss: 185.4666\n",
      "Epoch 206/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 196.0853 - val_loss: 188.9522\n",
      "Epoch 207/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 199.6605 - val_loss: 189.3194\n",
      "Epoch 208/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 197.8228 - val_loss: 186.0508\n",
      "Epoch 209/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 199.0123 - val_loss: 183.3329\n",
      "Epoch 210/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 196.7390 - val_loss: 186.3930\n",
      "Epoch 211/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 198.9567 - val_loss: 183.9339\n",
      "Epoch 212/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 197.1241 - val_loss: 189.9014\n",
      "Epoch 213/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 198.8625 - val_loss: 186.4021\n",
      "Epoch 214/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 194.7116 - val_loss: 182.7277\n",
      "Epoch 215/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 202.6199 - val_loss: 182.9244\n",
      "Epoch 216/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 195.2330 - val_loss: 186.2971\n",
      "Epoch 217/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 196.8933 - val_loss: 185.2268\n",
      "Epoch 218/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 196.7601 - val_loss: 183.5741\n",
      "Epoch 219/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 197.2915 - val_loss: 182.9814\n",
      "Epoch 220/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 194.4936 - val_loss: 187.7461\n",
      "Epoch 221/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 196.5857 - val_loss: 183.4951\n",
      "Epoch 222/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 195.8668 - val_loss: 187.6468\n",
      "Epoch 223/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 196.4238 - val_loss: 187.4769\n",
      "Epoch 224/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 195.5012 - val_loss: 186.4636\n",
      "Epoch 225/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 195.7657 - val_loss: 183.4674\n",
      "Epoch 226/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 191.5180 - val_loss: 186.0545\n",
      "Epoch 227/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 193.0887 - val_loss: 182.5614\n",
      "Epoch 228/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 194.7146 - val_loss: 186.2513\n",
      "Epoch 229/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 195.2858 - val_loss: 186.6164\n",
      "Epoch 230/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 196.0382 - val_loss: 186.0665\n",
      "Epoch 231/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 196.0812 - val_loss: 185.7466\n",
      "Epoch 232/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 195.3441 - val_loss: 183.9313\n",
      "Epoch 233/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 194.5625 - val_loss: 183.1299\n",
      "Epoch 234/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 193.2342 - val_loss: 186.3241\n",
      "Epoch 235/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 197.1558 - val_loss: 186.0608\n",
      "Epoch 236/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 193.5431 - val_loss: 182.1904\n",
      "Epoch 237/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 193.2504 - val_loss: 186.0091\n",
      "Epoch 238/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 192.3439 - val_loss: 182.0283\n",
      "Epoch 239/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 193.8545 - val_loss: 182.3344\n",
      "Epoch 240/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 194.9769 - val_loss: 183.0282\n",
      "Epoch 241/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 192.7412 - val_loss: 185.6085\n",
      "Epoch 242/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 193.7131 - val_loss: 184.2332\n",
      "Epoch 243/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 191.5750 - val_loss: 185.5476\n",
      "Epoch 244/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 194.5220 - val_loss: 187.8075\n",
      "Epoch 245/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 192.5418 - val_loss: 185.9706\n",
      "Epoch 246/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 194.9523 - val_loss: 182.6318\n",
      "Epoch 247/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 194.4606 - val_loss: 182.3708\n",
      "Epoch 248/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 192.0515 - val_loss: 181.8543\n",
      "Epoch 249/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 193.8241 - val_loss: 190.3079\n",
      "Epoch 250/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 192.9480 - val_loss: 186.3167\n",
      "Epoch 251/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 191.2930 - val_loss: 186.3547\n",
      "Epoch 252/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 191.4331 - val_loss: 185.2422\n",
      "Epoch 253/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 192.6102 - val_loss: 187.9211\n",
      "Epoch 254/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 191.2483 - val_loss: 185.9835\n",
      "Epoch 255/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 191.4888 - val_loss: 184.1752\n",
      "Epoch 256/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 193.1069 - val_loss: 181.5453\n",
      "Epoch 257/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.7527 - val_loss: 183.5804\n",
      "Epoch 258/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 190.6946 - val_loss: 183.0905\n",
      "Epoch 259/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 191.6408 - val_loss: 185.7465\n",
      "Epoch 260/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 189.6249 - val_loss: 184.1247\n",
      "Epoch 261/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 191.4408 - val_loss: 185.5810\n",
      "Epoch 262/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 190.5968 - val_loss: 183.7951\n",
      "Epoch 263/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.4512 - val_loss: 184.4630\n",
      "Epoch 264/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 189.2448 - val_loss: 185.3231\n",
      "Epoch 265/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.5794 - val_loss: 186.3829\n",
      "Epoch 266/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 189.2082 - val_loss: 185.0669\n",
      "Epoch 267/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 190.7654 - val_loss: 181.5289\n",
      "Epoch 268/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 191.5440 - val_loss: 185.8634\n",
      "Epoch 269/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.4125 - val_loss: 183.8708\n",
      "Epoch 270/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 189.9220 - val_loss: 182.1087\n",
      "Epoch 271/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 189.2075 - val_loss: 185.3199\n",
      "Epoch 272/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 186.7887 - val_loss: 183.4252\n",
      "Epoch 273/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 189.3764 - val_loss: 183.5523\n",
      "Epoch 274/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 190.9450 - val_loss: 186.9886\n",
      "Epoch 275/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.3880 - val_loss: 182.6214\n",
      "Epoch 276/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 189.0816 - val_loss: 183.9516\n",
      "Epoch 277/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.6813 - val_loss: 183.5367\n",
      "Epoch 278/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 190.1595 - val_loss: 185.2888\n",
      "Epoch 279/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 189.8082 - val_loss: 183.5079\n",
      "Epoch 280/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.4759 - val_loss: 185.4453\n",
      "Epoch 281/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.6607 - val_loss: 182.0953\n",
      "Epoch 282/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 189.4956 - val_loss: 185.1780\n",
      "Epoch 283/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 191.4587 - val_loss: 188.4938\n",
      "Epoch 284/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 189.0940 - val_loss: 180.1599\n",
      "Epoch 285/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.6517 - val_loss: 182.7702\n",
      "Epoch 286/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.8582 - val_loss: 182.9032\n",
      "Epoch 287/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.7573 - val_loss: 181.0225\n",
      "Epoch 288/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.1853 - val_loss: 185.9811\n",
      "Epoch 289/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.8816 - val_loss: 182.7425\n",
      "Epoch 290/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.5953 - val_loss: 185.7840\n",
      "Epoch 291/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.5200 - val_loss: 190.5150\n",
      "Epoch 292/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.4136 - val_loss: 183.7253\n",
      "Epoch 293/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.5533 - val_loss: 184.2873\n",
      "Epoch 294/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.6596 - val_loss: 183.8175\n",
      "Epoch 295/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 189.2637 - val_loss: 185.3561\n",
      "Epoch 296/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.1032 - val_loss: 189.2048\n",
      "Epoch 297/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 192.0663 - val_loss: 184.8936\n",
      "Epoch 298/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 189.5397 - val_loss: 183.2238\n",
      "Epoch 299/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.5630 - val_loss: 187.6673\n",
      "Epoch 300/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.8947 - val_loss: 181.5836\n",
      "Epoch 301/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.4960 - val_loss: 182.3745\n",
      "Epoch 302/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.3083 - val_loss: 185.1393\n",
      "Epoch 303/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.4258 - val_loss: 180.7964\n",
      "Epoch 304/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 184.8824 - val_loss: 183.9617\n",
      "Epoch 305/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 192.3027 - val_loss: 186.7797\n",
      "Epoch 306/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.9391 - val_loss: 182.3500\n",
      "Epoch 307/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.7883 - val_loss: 184.6772\n",
      "Epoch 308/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.7238 - val_loss: 183.7384\n",
      "Epoch 309/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.9549 - val_loss: 178.7497\n",
      "Epoch 310/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.6732 - val_loss: 179.4986\n",
      "Epoch 311/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 184.6161 - val_loss: 182.6392\n",
      "Epoch 312/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 186.1688 - val_loss: 190.1053\n",
      "Epoch 313/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.8804 - val_loss: 180.9244\n",
      "Epoch 314/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 186.3571 - val_loss: 183.6126\n",
      "Epoch 315/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.0053 - val_loss: 181.6822\n",
      "Epoch 316/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.6309 - val_loss: 182.7922\n",
      "Epoch 317/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.6354 - val_loss: 181.8612\n",
      "Epoch 318/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.1415 - val_loss: 184.0532\n",
      "Epoch 319/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.9913 - val_loss: 185.6207\n",
      "Epoch 320/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.0898 - val_loss: 185.2103\n",
      "Epoch 321/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.2695 - val_loss: 180.7680\n",
      "Epoch 322/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.5658 - val_loss: 183.7734\n",
      "Epoch 323/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 186.7064 - val_loss: 184.5913\n",
      "Epoch 324/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.0531 - val_loss: 188.3825\n",
      "Epoch 325/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.1102 - val_loss: 186.0601\n",
      "Epoch 326/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 184.2127 - val_loss: 186.7699\n",
      "Epoch 327/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 184.0776 - val_loss: 181.0851\n",
      "Epoch 328/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 183.5193 - val_loss: 179.2334\n",
      "Epoch 329/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 186.7516 - val_loss: 183.0033\n",
      "Epoch 330/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.6144 - val_loss: 184.0916\n",
      "Epoch 331/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 184.9319 - val_loss: 185.0758\n",
      "Epoch 332/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 184.4960 - val_loss: 182.5224\n",
      "Epoch 333/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 182.2185 - val_loss: 190.8561\n",
      "Epoch 334/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.6564 - val_loss: 183.5319\n",
      "Epoch 335/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 183.2521 - val_loss: 182.1743\n",
      "Epoch 336/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 184.1840 - val_loss: 183.8135\n",
      "Epoch 337/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.7678 - val_loss: 183.2958\n",
      "Epoch 338/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.8337 - val_loss: 185.1257\n",
      "Epoch 339/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 186.1181 - val_loss: 185.8900\n",
      "Epoch 340/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 190.2451 - val_loss: 196.7687\n",
      "Epoch 341/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 194.0467 - val_loss: 193.2847\n",
      "Epoch 342/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 194.4823 - val_loss: 189.8565\n",
      "Epoch 343/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 192.8112 - val_loss: 188.6433\n",
      "Epoch 344/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.3678 - val_loss: 188.3101\n",
      "Epoch 345/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 202.3109 - val_loss: 184.7990\n",
      "Epoch 346/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 192.0061 - val_loss: 191.6604\n",
      "Epoch 347/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 190.3564 - val_loss: 188.4357\n",
      "Epoch 348/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 189.2395 - val_loss: 186.8509\n",
      "Epoch 349/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.3820 - val_loss: 188.7701\n",
      "Epoch 350/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.6312 - val_loss: 186.6119\n",
      "Epoch 351/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 190.3158 - val_loss: 183.3625\n",
      "Epoch 352/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 186.7894 - val_loss: 181.1154\n",
      "Epoch 353/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.2914 - val_loss: 182.5545\n",
      "Epoch 354/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 192.3596 - val_loss: 188.7702\n",
      "Epoch 355/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 192.4123 - val_loss: 180.2243\n",
      "Epoch 356/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.6477 - val_loss: 182.1722\n",
      "Epoch 357/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 186.3941 - val_loss: 188.0143\n",
      "Epoch 358/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.7378 - val_loss: 186.7867\n",
      "Epoch 359/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 181.4554 - val_loss: 184.7977\n",
      "Epoch 360/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.0681 - val_loss: 187.2344\n",
      "Epoch 361/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 184.8320 - val_loss: 181.8476\n",
      "Epoch 362/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 182.2350 - val_loss: 187.3649\n",
      "Epoch 363/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 182.3545 - val_loss: 180.2875\n",
      "Epoch 364/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.9160 - val_loss: 187.6177\n",
      "Epoch 365/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 186.4518 - val_loss: 180.7562\n",
      "Epoch 366/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 194.5509 - val_loss: 184.9623\n",
      "Epoch 367/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.0078 - val_loss: 183.6281\n",
      "Epoch 368/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 183.0469 - val_loss: 186.2005\n",
      "Epoch 369/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 183.9201 - val_loss: 184.9090\n",
      "Epoch 370/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.2181 - val_loss: 197.5579\n",
      "Epoch 371/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 182.5390 - val_loss: 184.2531\n",
      "Epoch 372/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 183.2678 - val_loss: 184.3492\n",
      "Epoch 373/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.5423 - val_loss: 191.7603\n",
      "Epoch 374/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 188.9809 - val_loss: 185.3306\n",
      "Epoch 375/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 184.8209 - val_loss: 190.1743\n",
      "Epoch 376/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 187.2759 - val_loss: 185.5542\n",
      "Epoch 377/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 184.4619 - val_loss: 185.2558\n",
      "Epoch 378/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 182.6837 - val_loss: 184.0124\n",
      "Epoch 379/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 183.7543 - val_loss: 184.5002\n",
      "Epoch 380/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 183.9895 - val_loss: 184.2207\n",
      "Epoch 381/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 183.9765 - val_loss: 186.8959\n",
      "Epoch 382/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.7327 - val_loss: 186.6131\n",
      "Epoch 383/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 181.3634 - val_loss: 183.9272\n",
      "Epoch 384/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 183.9818 - val_loss: 189.2572\n",
      "Epoch 385/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.4953 - val_loss: 182.0265\n",
      "Epoch 386/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 181.3185 - val_loss: 181.3310\n",
      "Epoch 387/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 181.9206 - val_loss: 184.1507\n",
      "Epoch 388/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 185.8618 - val_loss: 186.4404\n",
      "Epoch 389/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 183.8858 - val_loss: 185.4272\n",
      "Epoch 390/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 182.4881 - val_loss: 187.4060\n",
      "Epoch 391/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 183.7579 - val_loss: 188.2017\n",
      "Epoch 392/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 183.3206 - val_loss: 183.5534\n",
      "Epoch 393/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 183.3799 - val_loss: 183.6756\n",
      "Epoch 394/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 184.6056 - val_loss: 184.6183\n",
      "Epoch 395/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 182.2058 - val_loss: 181.1895\n",
      "Epoch 396/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 181.7207 - val_loss: 184.5485\n",
      "Epoch 397/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 184.8626 - val_loss: 186.7847\n",
      "Epoch 398/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 182.3542 - val_loss: 183.8954\n",
      "Epoch 399/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 182.6945 - val_loss: 180.6367\n",
      "Epoch 400/400\n",
      "1130/1130 [==============================] - 8s 7ms/step - loss: 181.6404 - val_loss: 183.3132\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=400, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training and validation loss history\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv('./model_train_validate_loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This part is used to evaluate the model\n",
    "# It is normal that the two lines have different values\n",
    "# The thing that matters is they have the same \"trend\"\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "START, OFFSET = 2300, 80\n",
    "plt.plot(y_pred[:, -1][START: START + OFFSET], label='Pred')\n",
    "plt.plot(y_test[:, -1][START: START + OFFSET], label='Real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_5 (Batch (None, 30, 3)             12        \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                4608      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 5,775\n",
      "Trainable params: 5,769\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# Save model\n",
    "save_model(model, './lstm_covi_regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_5 (Batch (None, 30, 3)             12        \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                4608      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 5,775\n",
      "Trainable params: 5,769\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n",
      "4782.651\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To predict one data sample, do:\n",
    "# X_input should be a 30-days time slice\n",
    "# For a single day, \n",
    "# The final input shape: (1, 30, 3)\n",
    "# 1 for one input, 30 for the length of 30-days time slice, 3 for the  3-dimentional tuple for each day: [latitude, longitude, confirmed_cases]\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('./lstm_covid_regression')\n",
    "model.summary()\n",
    "preds = model.predict(X_test[0].reshape(1, 30, 3))\n",
    "pred_case_num = preds[0, -1]  # 0 to open the wrapper, -1 to get the number of predicted cases\n",
    "print(pred_case_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
